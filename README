ナイーブベイズ分類器:

Purpose(目的):
	ナイーブベイズ分類器の計算方法の改良と古典的な文書分類の実験

Requirements:

	Python 2.6 or greater, < 3.0:
	http://www.python.jp/Zope/download/pythoncore

	SQLite:
	http://www.sqlite.org/

	BeautifulSoup:
	http://www.crummy.com/software/BeautifulSoup/

	Yahoo!デベロッパーネットワーク日本語形態素解析:
	http://developer.yahoo.co.jp/webapi/jlp/ma/v1/parse.html

	※ Yahoo!のアプリケーションIDはAppIDと言うファイルを作ってそこにペースト/保存してください

How to Use(使い方):

    単にデフォルトの状態で試してみたい人は端末でnaivebayes.pyを走らせるだけです。
    同梱されているDATAフォルダ内のnaivebayes.dbは既に訓練済みです。
    その訓練用スクリプトがsampletrain.pyですが、これは別に使わなくて構いません。

    デフォルトのサンプル検証は次のような状況を用いています。
    フランス書院文庫
    http://www.france.jp/
    の(2011年8月6日現在での)「今月の新刊」の「一覧」から10冊以上書籍を刊行している作家を選び出し、
    その方々のサンプル文書を8個づつ訓練データとして用います。
    作家は次の7人の方々です。

    *田沼淳一
    *弓月誠
    *綺羅光
    *星野聖
    *御堂乱
    *藤崎玲
    *麻実克人

    この訓練用データとはまた別に、それぞれの作家が書いたサンプル2つづつを持ってきます。
    訓練データを元に、それら計14個のサンプルがそれぞれ「どの作家が書いたものか」正しく判別出来るのか、
    と言うのが「ナイーブベイズ分類器による文書分類実験」です。
    
    ご自分で試してみたいデータがあるのなら、naivebayes.dbを消去して、sampletrain.pyを参考に訓練してみてください。
    訓練用メソッドはtrain(訓練用データ)です。これでYahoo!の形態素解析を経由してデータがSQLiteに取り込まれます。
    なお、判別メソッドはclassifier(判別用データ)です。

Idea Behind This(考え方):

     * ナイーブベイズはベイズ統計?

       ナイーブベイズ分類器は逆確率法、と言う発想を元に作られていて、基本的には数式では、
       catをカテゴリー、wordを単語、Πを直積記号とすると、

	p(cat|word) ∝ p(cat)*Πp(word_i|cat)

       と表現されます。
       統計学者(特にベイジアンと言われる人たち)はあまりハッキリと言わないんですが、
       これは「逆確率法」であって、基本的には普通の「頻度主義での統計学」で出てくる範疇の考え方であって、
       「ベイズ統計学」とはあまり関係ありません。
       共通してるのは「ベイズの公式」

	p(B|A) = p(A|B)*p(B)/p(A)

       を基本として用いてるだけで、だからと言ってベイズ統計か、と言われると違うと思います。
       穿った見方をすると、「ベイズ」と言う単語が広まると、ベイズ統計系の予算が獲得しやすい、とか
       色んな理由があって、「違う」とは言わないんでしょう。
       (これは「アメリカ」主導の状況だと充分考えられます)

       他にも「ベイズ統計」じゃなくって「ベイズ理論」とか、あるいは「ベイズ確率」とか言う造語
       (実際、殆ど勝手な造語の範囲)がどんどん出てきて混乱してるような気がします。
       ナイーブベイズは普通/従来の頻度の統計の範疇であって、別に真新しいものではない、と言う基本は
       押さえておいた方が良い、とは思います。
       特にプログラマで「統計範疇を押さえなければならない」人はそう考えてた方が良いと思います。
       慌てて「ベイズ統計って何？」とベイズ統計の本を買っても十中八九「違う分野の本を読まされてる事に」
       気づくでしょう。

     *ゼロ頻度問題

	上記の通り、ナイーブベイズは単に逆確率のシンプルな応用です。
	書籍「集合知プログラミング」
	http://www.oreilly.co.jp/books/9784873113647/
	では「確率をひっくり返す」と表現されていましたが、まさしくやってる事はそれ「だけ」で、特に
	理論的に難しい事は行われていないのです(だからベイズ「理論」って程じゃないです)。
	ところで、ナイーブベイズの場合、カテゴリ中に現れる「単語」は全て互いに相関を持っていない、
	要するに「独立だ」と仮定しています。そこで独立な確率同士は定義上、

		p(A, B) = p(A)×p(B)

	となる(日本語で言うと「AとBが"同時に"起きる確率はAが起きる確率とBが起きる確率の積とする」)
	事を利用して、

		p(word_1, word2, ...|cat) = Πp(word_i|cat)

	を計算します。最終的にここで計算されたp(word_1, word_2, ...|cat)を逆確率で「ひっくり返して」

		p(cat|word) ≒ p(cat|word1, word2...)

	を計算する、と言うのがナイーブベイズです。
	ところで、積を計算する、と言った場合、じゃあ、適当なword_nが「一回も」データ上登場しなかった場合
	どうなるのか、と言う問題が生じます。と言うのも積を計算する以上、一個でも0があると

		Πp(word_i|cat) = 0

	になってしまいます。これは「ゼロ頻度問題」として知られています。そして、どうやってこれを回避
	するか、と言うスムージングと呼ばれる色んなアドホックな方法論が提唱されてきています。
	
　　*p(word_i|cat)の求め方こそが統計上一番重要な推定の問題

	上記はp(word_i|cat)に纏わるゼロ頻度問題と言う代表的な問題を取り上げました。そして、もっと大きく
	言うと、今まで見てきた通り、「ナイーブベイズ」が単に逆確率法である以上、大きな枠組みではそこで
	「推定精度に差が出る」って事はあり得ないんです。個人的に考える限り、ナイーブベイズと言う
	枠組み自体は「自明」なんでどーでもいいんですが、そうじゃなくって、データから「何をどのように
	言うか」と言うのは

		p(word_i|cat)

	をどう計算するかにかかってる、と思っています。ゼロ頻度問題もその中の一つが表面化した問題にしか
	過ぎません。

    *ベイズ推定

	ここでp(word_i|cat)を次のように捉えます。

	 θ = p(word_i|cat)

	今、p(word_i|cat) = θとして、最適なθを探せ、と言う問題にすり替えてみます。データyがあった場合、
	これは(ナイーブベイズとは関係がない)ベイズ推定の文脈では次のように表現されます。

	 p(θ|y) ∝ p(θ) * L(θ|y)

	この時のL(θ|y)を「尤度関数」あるいは単に「尤度」と呼びます。一般に、尤度は確率分布とは違い、総和は
	1になりません。数学的には普通の関数と捉えて構わないでしょう。
	尤度はデータyがあった場合、データyを固定してθを変数として捉えます。この問題設定の場合、例えばword_1が
	出現する確率をθ_w1、word_2が出現する確率をθ_w2....とする場合、まずは条件が、

	 ∑θ_wi = 1

	となる事。次に、データyは、word_1が出現した回数をy_w1、word_2が出現した回数をy_w2...とすると、

	 ∑y_wi = n ... nはデータ総数

	となります。そうすると、この場合、尤度L(θ|y)はθ_wiはそれぞれ互いに独立だとすると、

	 L(θ|y) = θ_w1 ^ y_w1 × θ_w2 ^ y_w2 × ... × θ_wn ^ y_wn = Π(θ_wi ^ y_wi)

	になります。つまり、

	 p(θ_wi|y_wi) ∝ p(θ_wi) * Π(θ_wi ^ y_wi)

	からθ_wiとしてもっともふさわしい(単語毎の)推定値を求めれば良いp(word_i|cat)になるのでは、と言うのが
	題意です。

    *共役事前分布とディリクレ分布

	上で設定した尤度は一般的には多項分布型になります。

		多項分布:
		http://aoki2.si.gunma-u.ac.jp/lecture/Bunpu/takou.html

	多項分布「型」と敢えて言ったのは上の設定、つまり、単語word_iの出現確率θ_wiが互いに独立だとすると、
	その尤度は計算上全てのθ_wi ^ y_wiの積になるから、で、流れはその通りで、
	多項分布の係数が特に必要ではないからです。
	また、多項分布の係数 n!/Πx_i!は総和が1になる為に必要なもので、よってこれは「正規化定数」と呼ばれます。
	一方、先にも解説した通り、尤度は一般に「総和が1になる」必要はありません。従って、ここでは多項分布
	そのものを糞真面目に当てはめて考える必要は無いのです。
	さて、そうなると

		p(θ_wi|y_wi) ∝ p(θ_wi) * Π(θ_wi ^ y_wi)

	で必要なのはp(θ_wi)がどんな分布なのか、と言う事です。
	なお、ベイズ推定に於いて、p(θ_wi)を一般に「事前分布」と呼び、計算結果として出てきた
	p(θ_wi|y_wi)を事後分布と呼びます。
	そして、ベイズ推定の文脈に於いてはp(θ_wi)を「何にしても構わない」と言う原則があります。
	と言うのも、ベイズ推定で扱う「確率」はいわゆる頻度などの「客観確率」ではなく、人の思い込みなどを
	表現した「主観確率」だからです。つまり、個人個人の経験や憶測に基づいた「分布」を選んできて
	構わない。
	この論理の流れから言うと「計算がラクな」分布を事前分布にしても構わない、と言う事になります。
	そのような分布を「☓☓分布に対する共役な事前分布」と呼びます。つまり、多項分布型の尤度に対して
	共役な事前分布を選んでくれば良い。
	多項分布に対する共役な事前分布をディリクレ分布といいます。

		ディリクレ分布:
		http://en.wikipedia.org/wiki/Dirichlet_distribution

	つまり、ディリクレ分布をDir(α_i)と表現すると、

		p(θ_wi|y_wi) ∝ Dir(α_i) * Π(θ_wi ^ y_wi)

	を計算する、のがθ_wi = p(word_i|cat)をどう求めるか、に繋がってくるのです。

    *ディリクレ分布の期待値

	さて、わざわざ共役な分布を選んできた理由と言うのは「計算が簡単だから」です。通常、尤度に共役な
	事前分布を選ぶと、計算結果である事後分布は事前分布と同じ形になり、単にパラメータをすげ替えれば
	済む、と言う特徴があります。
	この場合事前分布 => 事後分布の変換は

			 Dir(α_i) => Dir(α_i + y_wi)

	だけで済みます。
	これだけではちょっと抽象的過ぎるので、ディリクレ分布の期待値がどう変化するか見てみます。

		α_i / ∑α_i => (α_i + y_wi) / (∑α_i + n)

	つまり、例えば、θ_wi = p(word_i|cat)としたときに、ベイズ推定によるp(θ_wi|y_wi)をθ_wiの期待値と
	して採用するとすると、

		p(word_i|cat) = (α_i + y_wi) / (∑α_i + n)

	となると言う事です。
	なお、技術評論社の「機械学習 はじめよう:第3回　ベイジアンフィルタを実装してみよう」
	http://gihyo.jp/dev/serial/01/machine-learning/0003
	で用いられてる加算スムージング、ラプラス法ですが、実は上記のベイズ推定における期待値と全く同じ
	計算になります。暇な人は検算して確かめてみてください。記事中では

		「加算スムージングは単純であり簡単に実現できますが，一般的に精度が悪いことが知られています。」

	と記述されてはいますが、一方、p(word|cat)をベイズ推定した場合、非常に妥当な計算方法になってる事が
	分かると思います。

	※ 記事中のラプラス法のベイズ統計学的な解釈では、全てのα_iがα_i = 1となり、
	∑α_iが「単語の集合の要素の総数」を意味している。もちろん、ベイズ統計学的にはα_i毎に数値を変えても
	構わないが、あくまで全てのα_iを1とした場合、ラプラス法と一致する、と言う意味である。

    *MAP推定値

	この実装ではp(word_i|cat)の推定値として、ディリクレ分布の期待値ではなく、代わりに最頻値(モード)を
	用いています。と言うのも期待値はあくまで「平均」であり、必ずしも事後分布を最大化するθ_wiの値を
	意味しているわけではありません。
	一般に、事後分布を最大化するθ_wiの値をMAP(Maximum A Posteriori; 最大事後分布)推定値、と呼びます。
	最近のベイズ推定の流行りでは、このMAP推定値を用いると推定精度が良くなる、と言われています。
	ベイズ推定の文脈では事前分布 => 事後分布の推移に於いて、ディリクレ分布のモードは次のように変化
	します。

		(α_i - 1) / (∑α_i - k) => (α_i + y_wi -1) / (∑α_i + n - k)

	ここでkはiの総数(具体的には単語の集合の要素の総数)です。なお、α_i > 1と言う条件が付加されます。
	つまり、この実装では

		p(word|cat) = (α_i + y_wi -1) / (∑α_i + n - k)

	としています。
	もうちょっと具体的に言うと、デフォルトでα = α_i = 1.0001とし、

		p(word|cat) = (α - y_wi - 1) / (α × k + n - k)

	と言う推定値を用いています。

    *もう少し数学的に

	ある文書内の単語の出現確率をθ、ある文書があるカテゴリに属する確率をφ、データをyとして、
	θ、φ、yの同時分布は

		P(θ, φ, y)

	と表現されます。これは分解すると、

		P(θ, φ, y) = P(φ|θ, y) * P(y|θ) * P(θ)

	あるいは、

		P(θ, φ, y) = P(y|θ, φ) * P(θ|φ) * P(φ)  

	となります。左辺が同じ事から右辺同士は等号で結ぶ事が出来、

		P(φ|θ, y) * P(y|θ) * P(θ) = P(y|θ, φ) * P(θ|φ) * P(φ)
		∴ P(φ|θ, y) = P(y|θ, φ) * P(θ|φ) * P(φ) / P(θ, y)

	となります。従って、

		P(φ|θ, y) ∝ P(y|θ, φ) * P(θ|φ) * P(φ)
			   = [P(y|θ, φ) * P(θ|φ)] * P(φ)

	となります。今まではθ = p(word|cat)としてθの推定の問題としてたんですが、実のところ
	[]の内側の計算結果であるP(θ, y|φ)の推定問題として考えて良いでしょう。つまり、
	「あるカテゴリに於ける単語の出現率θとデータyの同時分布を求めよ」と言う問題を解いていたと
	言えるのです。
	左辺は「データyとwordの出現状況θから見たφ」と読み替える事が出来るので、元々
	のナイーブベイズの題意を満たしてる事は言えると思います。

    *階層ベイズ？

	上のように、ベイズの定理を拡張して、尤度と事前分布があり、それに事前分布の事前分布が
	掛け合わされてるような式で推定する技法で階層ベイズ推定、と言われるものがあります。

		Hierarachical Bayes model:
		http://en.wikipedia.org/wiki/Hierarchical_Bayes_model

	ただ、数式的には似てますが、上のはとてもじゃないですが、意味合い的には「階層ベイズモデル」
	とは言えないでしょう。
	と言うのも、数式変換はこの通りなんですが、一方[]の内側の事前分布P(θ|φ)はφの影響は直接
	受けてません。つまり、構造的には

		φ -> θ -> y

	と言う線形経路の構造を持ってないのです。これを考えると、実は上記の最初の式(φ, θ ,yの同時分布)は、

		P(φ, θ, y) =  P(y|θ, φ) * P(θ) * P(φ)

	と見なした方が良く、経路的には

		φ -
		   | - > y
		θ -

	と言う構造を持ってると考えられます。これは言い方を変えると

		親ノードを二つ(φとθ)もった合流経路

	と言えます。
	日本語で言うと「ある文書があるカテゴリに属してる」事象と「ある文書にある単語が現れる」事象それぞれ
	が結果として文書データに影響を与える、と言う事です。これは最初の仮定の構造としては凄く自然じゃない
	でしょうか？
	こう言う確率伝搬構造を仮定したモデルをベイジアンネットワーク

		Bayesian network:
		http://en.wikipedia.org/wiki/Bayesian_network

	と呼び、図らずしも一番単純なベイジアンネットワークを極めて単純化して計算した、とは言えるでしょう。
		
References:

	オリジナルソース:
	http://gihyo.jp/dev/serial/01/machine-learning/0003

	SQLite接続参考: 集合知プログラミング
	http://www.oreilly.co.jp/books/9784873113647/

	サンプル文書: フランス書院
	http://www.france.jp/